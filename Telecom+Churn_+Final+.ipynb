{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# To display all the columns and rows\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "# To map Empty Strings or numpy.inf as Na Values\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set Style\n",
    "sns.set(style = \"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=Indigo>Analysis Process</font>\n",
    "The analysis is divided into nine main parts:\n",
    "> 1. Data Sourcing or Data Understanding\n",
    "2. Data cleaning and Derived Metrics\n",
    "3. Exploratory Data Analysis (Univariate, Bivariate Analysis)\n",
    "4. Model Preparation\n",
    "     - Train and Test Data Split\n",
    "     - Data Normalization\n",
    "     - Handling Class Imbalance\n",
    "5. Model Building\n",
    "6. Residual Analysis of the Train Data\n",
    "7. Making Predictions\n",
    "8. Model Evaluation\n",
    "9. Final Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom = pd.read_csv('telecom_churn_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check the percentage of missing values in each column\n",
    "(telecom.isnull().sum()/ len(telecom)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference-from the above percentages we see that there are a few columns which have more than 70% of data missing. We can drop these columns as they would convey a very little information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should not drop the columns related to the recharge amounts as they would tell us who are high value customers. Thus to avoid doing that first we have to deal with the missing values in these columns -  \n",
    "\n",
    "Here is that we have to define **high-value customers** as - **Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase)**.\n",
    "\n",
    "- total_rech_data_6  - 74.8467 % missing values\n",
    "- total_rech_data_7  - 74.4287 % missing values\n",
    "- av_rech_amt_data_6 - 74.8467 % missing values\n",
    "- av_rech_amt_data_7 - 74.4287 % missing values\n",
    "\n",
    "We have 2 more columns as -\n",
    "\n",
    "- date_of_last_rech_data_6 - 74.8467 % missing values\n",
    "- date_of_last_rech_data_7 - 74.4287 % missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are dealing with users churn - where Customers who have not done any usage, either incoming or outgoing - in terms of calls, internet etc. over a period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Duplicate check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom['mobile_number'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom.set_index('mobile_number',inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Derivied columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Total rech columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have values for 'total_rech_data_' and 'av_rech_amt_data_' for the particular months 6, 7, 8 & 9. Using these 2 values we can derive new feature for the respective months called total_data_rech_amt_ which equals\n",
    "total_rech_data_ * av_rech_amt_data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new feature would help in computing the total data recharge amount - 'total_data_rech_amt_' for the months 6, 7, 8 & 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no recharge is done for the month then 'total_rech_data_6' and 'date_of_last_rech_data_6' both will have null values in the\n",
    "# same row. So, if index match for record, we can impute the 'total_rech_data_6' and 'av_rech_amt_data_6' with 0. Same for the\n",
    "# other two months as well.\n",
    "\n",
    "total_recharge_6_index = telecom['total_rech_data_6'].isnull()\n",
    "date_of_last_recharge_6_index = telecom['date_of_last_rech_data_6'].isnull()\n",
    "\n",
    "if total_recharge_6_index.equals(date_of_last_recharge_6_index):\n",
    "    print(' NULL values for month 6 are imputed')\n",
    "    telecom['total_rech_data_6'].fillna(0, inplace=True)\n",
    "    telecom['av_rech_amt_data_6'].fillna(0, inplace=True)\n",
    "\n",
    "total_rech_7_index = telecom['total_rech_data_7'].isnull()\n",
    "date_of_last_rech_7_index = telecom['date_of_last_rech_data_7'].isnull()\n",
    "\n",
    "if total_rech_7_index.equals(date_of_last_rech_7_index):\n",
    "    print(' NULL values for month 7 are imputed')\n",
    "    telecom['total_rech_data_7'].fillna(0, inplace=True)\n",
    "    telecom['av_rech_amt_data_7'].fillna(0, inplace=True)\n",
    "\n",
    "total_rech_8_index = telecom['total_rech_data_8'].isnull()\n",
    "date_of_last_rech_8_index = telecom['date_of_last_rech_data_8'].isnull()\n",
    "\n",
    "if total_rech_8_index.equals(date_of_last_rech_8_index):\n",
    "    print(' NULL values for month 8 are imputed')\n",
    "    telecom['total_rech_data_8'].fillna(0, inplace=True)\n",
    "    telecom['av_rech_amt_data_8'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference- We see that the two indexes object are equal and we can safely conclude that no data recharge was done for months 6, 7 and 8 & the missing values can be imputed with 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rech_cols = []\n",
    "for x in telecom.columns.tolist():\n",
    "    if (('rech' in x) or ('amt' in x) or ('arpu' in x)) and (('date' not in x) and ('day' not in x)):\n",
    "        rech_cols.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom[rech_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom[rech_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "telecom[['total_rech_num_6','total_rech_amt_6','total_rech_data_6','av_rech_amt_data_6']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = telecom.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy[rech_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the describe we can see that it is safe to assume to replace null with 0 for all the rech_amt,rech_count and max columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with more than 70% data missing\n",
    "\n",
    "cols_with_70_percent_missing_data = ((telecom.isnull().sum()/ len(telecom)) * 100) >= 70\n",
    "cols_with_70_percent_missing_data.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference - Our objective is to only find out the churn rate of the High valued customers ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom['total_amt_6'] = copy['total_rech_amt_6']+copy['total_rech_data_6']*copy['av_rech_amt_data_6']\n",
    "telecom['total_amt_7'] = copy['total_rech_amt_7']+copy['total_rech_data_7']*copy['av_rech_amt_data_7']\n",
    "telecom['total_amt_8'] = copy['total_rech_amt_8']+copy['total_rech_data_8']*copy['av_rech_amt_data_8']\n",
    "telecom['total_amt_9'] = copy['total_rech_amt_9']+copy['total_rech_data_9']*copy['av_rech_amt_data_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom['total_amt_6_7'] =telecom['total_amt_6']+telecom['total_amt_7']\n",
    "telecom['total_amt_8_9'] =telecom['total_amt_8']+telecom['total_amt_9']\n",
    "telecom['total_amt_6_9'] = telecom['total_amt_6']+telecom['total_amt_7'] +telecom['total_amt_8']+telecom['total_amt_9'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Av amount -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom['total_rech_count_6'] = copy['av_rech_amt_data_6']+copy['total_rech_num_6']\n",
    "telecom['total_rech_count_7'] = copy['av_rech_amt_data_7']+copy['total_rech_num_7']\n",
    "telecom['total_rech_count_8'] = copy['av_rech_amt_data_8']+copy['total_rech_num_8']\n",
    "telecom['total_rech_count_9'] = copy['av_rech_amt_data_9']+copy['total_rech_num_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom['av_amt_6_7'] = telecom['total_amt_6_7']/(telecom['total_rech_count_6']+telecom['total_rech_count_7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom['av_amt_6_7'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom[telecom['av_amt_6_7'].isnull()][['av_amt_6_7','total_rech_count_6','total_rech_count_7','total_amt_6_7']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the rows where the average amount is nan, the total recharges are 0.Therefore replacing null with 0\n",
    "telecom['av_amt_6_7'] = telecom['av_amt_6_7'].replace(np.nan,0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom['av_amt_6_7'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 High value customers\n",
    "As we are concerned with high-value customers. Define high-value customers as those customers who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those customers who have spend an average amount more than the 70 percentile of average amount in the first two months are considered as HVCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc = telecom[telecom['av_amt_6_7']>telecom['av_amt_6_7'].quantile(0.7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference-So after filtering out the high-value customers we are left with 29993 observations. The shape of the telecom dataset is now (29993, 237)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 Tagging customers who have Churned-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those customers who have not made any calls or received any calls and not have used any form of mobile data are considered as churned customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### null value check for those 4 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc[['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc[['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "taking the maximum among the 4 columns and equating it to 0 to identify the churn customers. Single the max value for such customers will be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_indices = telecom_hvc[telecom_hvc[['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9']].max(axis =1) == 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(churn_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc.loc[churn_indices,['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc['churn'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc.loc[churn_indices,'churn'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc['churn'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Data Cleaning & Missing Values Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(telecom.isnull().sum(axis = 0)/ len(telecom)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping all columns corresponding to churn phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns that have more than 40% data missing.\n",
    "total_cols_with_atleast_40_percent_missing_data = ((telecom.isnull().sum()/ len(telecom)) * 100) >= 40\n",
    "total_cols_with_atleast_40_percent_missing_data.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the column/feature names which have more than 40% missing data.\n",
    "\n",
    "cols_names_with_40_percent_missing_value = total_cols_with_atleast_40_percent_missing_data[\n",
    "    total_cols_with_atleast_40_percent_missing_data > 0.40].index\n",
    "\n",
    "cols_names_with_40_percent_missing_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets examine the above columns (with >40% missing values) one by one -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total NULL values for max_rech_data_6 columns is - \", telecom.max_rech_data_6.isnull().sum())\n",
    "print(\"Total NULL values for max_rech_data_7 columns is - \", telecom.max_rech_data_7.isnull().sum())\n",
    "print(\"Total NULL values for max_rech_data_8 columns is - \", telecom.max_rech_data_8.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference - Missing values can be infered as No recharge was done and can be imputed with 0 for all the 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NA values with 0\n",
    "for col in ['max_rech_data_6', 'max_rech_data_7', 'max_rech_data_8']:\n",
    "    telecom[col].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the columns that have more than 40% missing values one way or the other relate to data/mobile internet usage for the months 6, 7 & 8 respectively. One important observation that comes out is that for a particular month all these values have same number of missing values. The below data tells us the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_phase_cols = []\n",
    "for x in telecom_hvc.columns.tolist():\n",
    "    if '_9' in x:\n",
    "        churn_phase_cols.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(churn_phase_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(churn_phase_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_1 =  telecom_hvc.drop(churn_phase_cols,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(telecom_hvc.shape[1]-telecom_hvc_1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null value check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns = (telecom_hvc_1.isnull().sum()/len(telecom_hvc_1))[telecom_hvc_1.isnull().sum()/len(telecom_hvc_1) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(null_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting into percentage\n",
    "null_columns = null_columns*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns[null_columns>20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(null_columns[null_columns>20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_2 = telecom_hvc_1.drop(null_columns[null_columns>20].index.tolist(),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(telecom_hvc_1.shape[1]-telecom_2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns.drop(null_columns[null_columns>20].index.tolist(),inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(null_columns.sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all the columns have less than 6% of null values.Lets check if there any rows which contain large number of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values_rows = telecom_2.isnull().sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values_rows[null_values_rows > 0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_2.loc[null_values_rows[null_values_rows > 0].index,'churn'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__There rows with null values have about 30% of total churn customers. Therefore dropping these rows will result in significant loss of churn data.Therefore we have to impute these values instead of dropping__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Null values in the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_2.select_dtypes(include = 'object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all are date columns hence converting them into datetime\n",
    "for column in telecom_2.select_dtypes(include = 'object').columns:\n",
    "    telecom_2[column] = telecom_2[column].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_2[null_columns.index].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(telecom_2['last_date_of_month_6'].value_counts())\n",
    "print(telecom_2['last_date_of_month_7'].value_counts())\n",
    "print(telecom_2['last_date_of_month_8'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping last date columns as they only signify the last date of month.\n",
    "telecom_hvc_3 = telecom_2.drop(['last_date_of_month_6','last_date_of_month_7','last_date_of_month_8'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(telecom_2.shape[1]-telecom_hvc_3.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date_of_last_rech columns\n",
    "# creating new columns called is_rech for each month which shows if a customer has recharged in the particular month or not.\n",
    "telecom_hvc_3['is_rech_6'] = 1\n",
    "telecom_hvc_3['is_rech_7'] = 1\n",
    "telecom_hvc_3['is_rech_8'] = 1\n",
    "\n",
    "telecom_hvc_3.loc[telecom_hvc_3['date_of_last_rech_6'].isnull(),'is_rech_6'] = 0\n",
    "telecom_hvc_3.loc[telecom_hvc_3['date_of_last_rech_7'].isnull(),'is_rech_7'] = 0\n",
    "telecom_hvc_3.loc[telecom_hvc_3['date_of_last_rech_8'].isnull(),'is_rech_8'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping  Date_of_last_rech columns as they only signify the last date of month.\n",
    "telecom_4 = telecom_hvc_3.drop(['date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(telecom_hvc_3.shape[1]-telecom_4.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns.drop(['date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8','last_date_of_month_7','last_date_of_month_8'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_4[null_columns.index].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight 1-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference-We can also see an observation with mobile internet usage having a missing value would mostly mean that the customer is not using that particular service. This could also mean that the person would not be using any other kind of  add-on services that would require a mobile internet pack.\n",
    "Also with this inference we can impute the missing values related to columns for mobile data with 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing all null values with 0\n",
    "telecom_hvc_5 = telecom_4.replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(telecom_hvc_5.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_5.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping columns with single value in the entire column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dictionary with column and number of unique values\n",
    "unique_value_dict = {}\n",
    "for x in telecom_hvc_5.columns.tolist():\n",
    "    l = len(telecom_hvc_5[x].value_counts())\n",
    "    unique_value_dict[x] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df = pd.DataFrame.from_dict(unique_value_dict,orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df[unique_df[0] == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the columns with only one unique value\n",
    "telecom_hvc_6 = telecom_hvc_5.drop(unique_df[unique_df[0] == 1].index,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "telecom_hvc_6.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping columns with high percentage of single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dictionary with column and number of unique values\n",
    "high_value_dict = {}\n",
    "for x in telecom_hvc_6.columns.tolist():\n",
    "    l = round(100*(telecom_hvc_6[x].value_counts().iloc[0]/len(telecom_hvc_6)))\n",
    "    high_value_dict[x] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_df = pd.DataFrame.from_dict(high_value_dict,orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_df.sort_values(by = 0,ascending = False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the columns with high percentage of single value\n",
    "telecom_hvc_6 = telecom_hvc_6.drop(high_df[high_df[0] >= 99].index,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming month name :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6.rename(columns = {'aug_vbc_3g':'vbc_3g_8','jun_vbc_3g':'vbc_3g_6','jul_vbc_3g':'vbc_3g_7','sep_vbc_3g':'vbc_3g_9'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6.drop('vbc_3g_9',axis =1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for zero variance -\n",
    "Let's check on the other columns now. But before proceeding with the missing value analysis of other columns let's check if we have any columns with zero variance (as this would be very important while conducting **Principal Component Analysis**). For the problem at hand if any variable has zero variance (all same values) it would have no impact during the analysis. We will drop duch columns.\n",
    "\n",
    "We can use var() for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Variance  - df.var() method\n",
    "columns_with_0_variance = telecom.var() == 0\n",
    "print(\"Total columns with ZERO variance are - \", columns_with_0_variance.sum())\n",
    "column_name_with_0_variance = columns_with_0_variance[columns_with_0_variance == 1].index\n",
    "print(column_name_with_0_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the column for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_null_values = telecom.columns[telecom.isna().any()].tolist()\n",
    "print(columns_with_null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Month 6\n",
    "cols_with_null_values_for_month_6 = [col for col in columns_with_null_values if '_6' in col]\n",
    "print(telecom[cols_with_null_values_for_month_6].info())\n",
    "\n",
    "# Month 7\n",
    "cols_with_null_values_for_month_7 = [col for col in columns_with_null_values if '_7' in col]\n",
    "print(telecom[cols_with_null_values_for_month_7].info())\n",
    "\n",
    "# Month 8\n",
    "cols_with_null_values_for_month_8 = [col for col in columns_with_null_values if '_8' in col]\n",
    "print(telecom[cols_with_null_values_for_month_8].info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference- . On mapping these attributes using the data dictionary we know that all these attributes point to the minutes of usage calls (incoming/outgoing) made within the network or to outside network.\n",
    "\n",
    "So if any value is missing means that the minutes of usage is missing and can be imputed with 0.\n",
    "\n",
    "The same would apply for the columns pointing to months 7 & 8 aswell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight 2-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also one more insight that the above figures give us is that towards the 8th month the number of missing values increases and this could possibly mean that the customer might have stopped using these services. This could be an indication that the particular customer is more likely to churn.\n",
    "\n",
    "So not using the services means the minutes of usage can be imputed with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with 0 excluding the date columns.\n",
    "for column in columns_with_null_values:\n",
    "    if \"date_of_last_rech\" not in column:\n",
    "        telecom[column].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# description for missing values-\n",
    "print(telecom.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insight 3 -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the index of missing values are equal then we can assume that they are for same record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the index are equal for all entries, then we can confirm that the missing values are all from the same observations/index.\n",
    "\n",
    "# Month 6\n",
    "arpu_3g_6_index = telecom['arpu_3g_6'].isnull()\n",
    "night_pck_user_6_index = telecom['night_pck_user_6'].isnull()\n",
    "count_rech_2g_6_index = telecom['count_rech_2g_6'].isnull()\n",
    "count_rech_3g_6_index = telecom['count_rech_3g_6'].isnull()\n",
    "fb_user_6_index = telecom['fb_user_6'].isnull()\n",
    "\n",
    "if arpu_3g_6_index.equals(night_pck_user_6_index) & night_pck_user_6_index.equals(count_rech_2g_6_index) & count_rech_2g_6_index.equals(count_rech_3g_6_index) &  count_rech_3g_6_index.equals(fb_user_6_index):\n",
    "    print('The indexes for NULL values for month 6 are equal')\n",
    "\n",
    "# Month 7\n",
    "arpu_3g_7_index = telecom['arpu_3g_7'].isnull()\n",
    "night_pck_user_7_index = telecom['night_pck_user_7'].isnull()\n",
    "count_rech_2g_7_index = telecom['count_rech_2g_7'].isnull()\n",
    "count_rech_3g_7_index = telecom['count_rech_3g_7'].isnull()\n",
    "fb_user_7_index = telecom['fb_user_7'].isnull()\n",
    "\n",
    "if arpu_3g_7_index.equals(night_pck_user_7_index) & night_pck_user_7_index.equals(count_rech_2g_7_index) & count_rech_2g_7_index.equals(count_rech_3g_7_index) &  count_rech_3g_7_index.equals(fb_user_7_index):\n",
    "    print('The indexes for NULL values for month 7 are equal')\n",
    "\n",
    "# Month 8\n",
    "arpu_3g_8_index = telecom['arpu_3g_8'].isnull()\n",
    "night_pck_user_8_index = telecom['night_pck_user_8'].isnull()\n",
    "count_rech_2g_8_index = telecom['count_rech_2g_8'].isnull()\n",
    "count_rech_3g_8_index = telecom['count_rech_3g_8'].isnull()\n",
    "fb_user_8_index = telecom['fb_user_8'].isnull()\n",
    "\n",
    "if arpu_3g_8_index.equals(night_pck_user_8_index) & night_pck_user_8_index.equals(count_rech_2g_8_index) & count_rech_2g_8_index.equals(count_rech_3g_8_index) &  count_rech_3g_8_index.equals(fb_user_8_index):\n",
    "    print('The indexes for NULL values for month 8 are equal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing the revenue columns-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom['arpu_6'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom['arpu_7'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom['arpu_8'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference-The above statistical description of the ARPU - Average Revenue Per User column gives some important insight. The MINIMUM value for all the arpu related columns are NEGATIVE. Now as per the defination of ARPU which says -\n",
    "\n",
    "\"The average revenue per user is the average billing per customer earned by the telecom company every month\".Now the revenue generated from a user cannot be negative. If a customer is not using any services then apru for the person would be zero (rather that being negative). Now if arpu is negative for any row, then that would mean that is a wrong/corrupt data. It will be of no use to us for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index where the arpu values for month 6 are less than 0 -\n",
    "\n",
    "arpu_6_index = (telecom['arpu_6'] < 0)\n",
    "print('Total observations with negative arpu values for month 6 -', arpu_6_index.sum())\n",
    "\n",
    "# Index where the arpu values for month 7 are less than 0 -\n",
    "\n",
    "arpu_7_index = (telecom['arpu_7'] < 0)\n",
    "print('Total observations with negative arpu values for month 7 -', arpu_7_index.sum())\n",
    "\n",
    "# Index where the arpu values for month 8 are less than 0 -\n",
    "\n",
    "arpu_8_index = (telecom['arpu_8'] < 0)\n",
    "print('Total observations with negative arpu values for month 8 -', arpu_8_index.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's delete the observations with negative arpu values. \n",
    "\n",
    "telecom= telecom[(telecom['arpu_6'] >= 0) & (telecom['arpu_7'] >= 0) & (telecom['arpu_8'] >= 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now go ahead with EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical column check-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if there are any categorical variables in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_cols = ['night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8', 'fb_user_6', 'fb_user_7', 'fb_user_8']\n",
    "\n",
    "telecom[category_cols] =telecom[category_cols].astype(int)\n",
    "\n",
    "telecom[category_cols].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Univariate analysis-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Univariate Analysis - Ordered Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Function to add data labels in the graph\n",
    "\n",
    "def def_data_labels(ax, spacing = 5):\n",
    "\n",
    "    # For each bar: Place a label\n",
    "    for rect in ax.patches:\n",
    "        # Get X and Y placement of label from rect.\n",
    "        y_value = rect.get_height()\n",
    "        x_value = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "        # Number of points between bar and label. Change to your liking.\n",
    "        space = spacing\n",
    "        # Vertical alignment for positive values\n",
    "        va = 'bottom'\n",
    "\n",
    "        # If value of bar is negative: Place label below bar\n",
    "        if y_value < 0:\n",
    "            # Invert space to place label below\n",
    "            space *= -1\n",
    "            # Vertically align label at top\n",
    "            va = 'top'\n",
    "\n",
    "        # Use Y value as label and format number with one decimal place\n",
    "        label = \"{:.2f}%\".format(y_value)\n",
    "\n",
    "        # Create annotation\n",
    "        plt.annotate(\n",
    "            label,                        # Use `label` as label\n",
    "            (x_value, y_value),           # Place label at end of the bar\n",
    "            xytext = (0, space),          # Vertically shift label by `space`\n",
    "            textcoords = \"offset points\", # Interpret `xytext` as offset in points\n",
    "            ha = 'center',                # Horizontally center label\n",
    "            va = va)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Plot Analysis of Ordered categorical variables vs Percentage Rate\n",
    "category_col = ['night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8', 'fb_user_6', 'fb_user_7', 'fb_user_8']\n",
    "counter = 1\n",
    "\n",
    "plt.figure(figsize = (15, 12))\n",
    "\n",
    "for col_list in category_col:\n",
    "        \n",
    "    series = round(((telecom[col_list].value_counts(dropna = False))/(len(telecom[col_list])) * 100), 2)\n",
    "\n",
    "    plt.subplot(2, 3, counter)\n",
    "    ax = sns.barplot(x = series.index, y = series.values, order = series.sort_index().index)\n",
    "    plt.xlabel(col_list, labelpad = 15)\n",
    "    plt.ylabel('Percentage Rate', labelpad = 10)\n",
    "\n",
    "    # Call Custom Function\n",
    "    def_data_labels(ax)\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "del category_col, counter, ax\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.3)\n",
    "plt.subplots_adjust(wspace = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference -From the `night_pck_user` variables, it can be inferred that almost 99% of users are not using nightly pack in all the 3 months.The number of users using the pack is a very small percentage of users.The % of users not using the pack is slightly going up .\n",
    "\n",
    "However users are using facebook(fb) `fb_user` in equal percentage and it is observed that as the number of months progress, there is a small decline in the fb usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(telecom_hvc_6.churn.value_counts())\n",
    "print(100*telecom_hvc_6.churn.value_counts()/len(telecom_hvc_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 10% of the high value customers are churn customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recharge amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total_amt including data recharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "telecom_hvc_6[['total_amt_6','total_amt_7','total_amt_8']].describe(percentiles = [0.25,0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['total_amt_6','total_amt_7','total_amt_8','churn']].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the total rech amount, slight down trend but not too steep of a decline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### total_amt vs churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "i = 1\n",
    "for col in ['total_amt_6','total_amt_7','total_amt_8']:\n",
    "    plt.subplot(1,3,i)\n",
    "    sns.boxplot(x = 'churn',y = col,data = telecom_hvc_6,showfliers = False)\n",
    "    i = i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "telecom_hvc_6[['total_amt_6','total_amt_7','total_amt_8','churn']].groupby('churn').median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar trend for the total rech amount as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  >> Derived column: difference from consecutive months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6['diff_amt_6_7'] = telecom_hvc_6['total_amt_6'] - telecom_hvc_6['total_amt_7']\n",
    "telecom_hvc_6['diff_amt_7_8'] = telecom_hvc_6['total_amt_7'] - telecom_hvc_6['total_amt_8']\n",
    "telecom_hvc_6['diff_amt_6_8'] = telecom_hvc_6['total_amt_6'] - telecom_hvc_6['total_amt_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['diff_amt_6_7','diff_amt_7_8','diff_amt_6_8','churn']].groupby('churn').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference-We can see that in case of the non churn customers the mean of the difference in recharges is negative indicating that the recharge amount is increasing with number of months, where as we can see that there is a huge decrease in case of churn customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of recharges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "telecom_hvc_6[['total_rech_count_6','total_rech_count_7','total_rech_count_8']].describe(percentiles = [0.25,0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['total_rech_count_6','total_rech_count_7','total_rech_count_8']].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference-More or less same count can be observed through the 3 months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### total_rech_count vs churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "i = 1\n",
    "for col in ['total_rech_count_6','total_rech_count_7','total_rech_count_8']:\n",
    "    plt.subplot(1,3,i)\n",
    "    sns.boxplot(x = 'churn',y = col,data = telecom_hvc_6,showfliers = False)\n",
    "    i = i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['total_rech_count_6','total_rech_count_7','total_rech_count_8','churn']].groupby('churn').median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number recharges also follow the similar trend of the recharge amounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >>  Derived column: difference from consecutive months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6['diff_count_6_7'] = telecom_hvc_6['total_rech_count_6'] - telecom_hvc_6['total_rech_count_7']\n",
    "telecom_hvc_6['diff_count_7_8'] = telecom_hvc_6['total_rech_count_7'] - telecom_hvc_6['total_rech_count_8']\n",
    "telecom_hvc_6['diff_count_6_8'] = telecom_hvc_6['total_rech_count_6'] - telecom_hvc_6['total_rech_count_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['diff_count_6_7','diff_count_7_8','diff_count_6_8','churn']].groupby('churn').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference-We can see that in case of the non churn customers the mean of the difference in recharge counts is negative indicating that the recharge count is increasing with number of months, where as we can see that there is a decrease in case of churn customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incoming calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in telecom_hvc_6.columns if 'ic' in col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total_incoming calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "telecom_hvc_6[['total_ic_mou_6','total_ic_mou_7','total_ic_mou_8']].describe(percentiles = [0.25,0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['total_ic_mou_6','total_ic_mou_7','total_ic_mou_8']].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference-No observable trend, it is almost constant . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### total_ic vs churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "i = 1\n",
    "for col in ['total_ic_mou_6','total_ic_mou_7','total_ic_mou_8']:\n",
    "    plt.subplot(1,3,i)\n",
    "    sns.boxplot(x = 'churn',y = col,data = telecom_hvc_6,showfliers = False)\n",
    "    i = i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['total_ic_mou_6','total_ic_mou_7','total_ic_mou_8','churn']].groupby('churn').median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference-We can see that the non churn customers have almost similar median through the period where as for churn customers it is almost 0 in the 8th month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Derived column: difference from consecutive months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6['diff_total_ic_6_7'] = telecom_hvc_6['total_ic_mou_6'] - telecom_hvc_6['total_ic_mou_7']\n",
    "telecom_hvc_6['diff_total_ic_7_8'] = telecom_hvc_6['total_ic_mou_7'] - telecom_hvc_6['total_ic_mou_8']\n",
    "telecom_hvc_6['diff_total_ic_6_8'] = telecom_hvc_6['total_ic_mou_6'] - telecom_hvc_6['total_ic_mou_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['diff_total_ic_6_7','diff_total_ic_7_8','diff_total_ic_6_8','churn']].groupby('churn').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference-We can see that for churn customers the total incoming calls seems to decrease consistently as we move to action phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local incoming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "telecom_hvc_6[['loc_ic_mou_6','loc_ic_mou_7','loc_ic_mou_8']].describe(percentiles = [0.25,0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['loc_ic_mou_6','loc_ic_mou_7','loc_ic_mou_8']].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference - We can see No observable trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loc_ic vs churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "i = 1\n",
    "for col in ['loc_ic_mou_6','loc_ic_mou_7','loc_ic_mou_8']:\n",
    "    plt.subplot(1,3,i)\n",
    "    sns.boxplot(x = 'churn',y = col,data = telecom_hvc_6,showfliers = False)\n",
    "    i = i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['loc_ic_mou_6','loc_ic_mou_7','loc_ic_mou_8','churn']].groupby('churn').median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the non churn customers have almost similar median through the period where as for churn customers it is almost 0 in the 8th month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### std incoming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "telecom_hvc_6[['std_ic_mou_6','std_ic_mou_7','std_ic_mou_8']].describe(percentiles = [0.25,0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['std_ic_mou_6','std_ic_mou_7','std_ic_mou_8']].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No observable trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### std_ic Vs churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "i = 1\n",
    "for col in ['std_ic_mou_6','std_ic_mou_7','std_ic_mou_8']:\n",
    "    plt.subplot(1,3,i)\n",
    "    sns.boxplot(x = 'churn',y = col,data = telecom_hvc_6,showfliers = False)\n",
    "    i = i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['std_ic_mou_6','std_ic_mou_7','std_ic_mou_8','churn']].groupby('churn').median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the non churn customers have almost similar median through the period where as for churn customers it is almost 0 in the 8th month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### roam incoming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "telecom_hvc_6[['roam_ic_mou_6','roam_ic_mou_7','roam_ic_mou_8']].describe(percentiles = [0.25,0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['roam_ic_mou_6','roam_ic_mou_7','roam_ic_mou_8']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### roam_og Vs churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "i = 1\n",
    "for col in ['roam_ic_mou_6','roam_ic_mou_7','roam_ic_mou_8']:\n",
    "    plt.subplot(1,3,i)\n",
    "    sns.violinplot(x = 'churn',y = col,data = telecom_hvc_6,showfliers = False)\n",
    "    i = i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['roam_ic_mou_6','roam_ic_mou_7','roam_ic_mou_8','churn']].groupby('churn').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The customers who have churned have large amount of roaming incoming calls than the non churned customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out going calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[col for col in telecom_hvc_6.columns if 'oc' in col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total_outgoing calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "telecom_hvc_6[['total_og_mou_6','total_og_mou_7','total_og_mou_8']].describe(percentiles = [0.25,0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['total_og_mou_6','total_og_mou_7','total_og_mou_8']].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first two months there is a slight decrease where as there is a sharp decrease from 7 th to 8th month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### total_og vs churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "i = 1\n",
    "for col in ['total_og_mou_6','total_og_mou_7','total_og_mou_8']:\n",
    "    plt.subplot(1,3,i)\n",
    "    sns.boxplot(x = 'churn',y = col,data = telecom_hvc_6,showfliers = False)\n",
    "    i = i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['total_og_mou_6','total_og_mou_7','total_og_mou_8','churn']].groupby('churn').median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the intial month the median oc calls is similar, but it kept decreasing with very steep decent through the months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Derived column: difference from consecutive months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6['diff_total_og_6_7'] = telecom_hvc_6['total_og_mou_6'] - telecom_hvc_6['total_og_mou_7']\n",
    "telecom_hvc_6['diff_total_og_7_8'] = telecom_hvc_6['total_og_mou_7'] - telecom_hvc_6['total_og_mou_8']\n",
    "telecom_hvc_6['diff_total_og_6_8'] = telecom_hvc_6['total_og_mou_6'] - telecom_hvc_6['total_og_mou_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['diff_total_og_6_7','diff_total_og_7_8','diff_total_og_6_8','churn']].groupby('churn').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference-For the churn customers the decrease in total out calls is very high, which says that the churn customers tend to make less number of calls in the action phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local outgoing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "telecom_hvc_6[['loc_og_mou_6','loc_og_mou_7','loc_og_mou_8']].describe(percentiles = [0.25,0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['loc_og_mou_6','loc_og_mou_7','loc_og_mou_8']].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference - We see in the intial two months the og calls are almost same where as there is a slight dip in the final month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loc_og vs churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "i = 1\n",
    "for col in ['loc_og_mou_6','loc_og_mou_7','loc_og_mou_8']:\n",
    "    plt.subplot(1,3,i)\n",
    "    sns.boxplot(x = 'churn',y = col,data = telecom_hvc_6,showfliers = False)\n",
    "    i = i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['loc_og_mou_6','loc_og_mou_7','loc_og_mou_8','churn']].groupby('churn').median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference-We can see that the non churn customers have almost similar median through the period where as for churn customers it is almost 0 in the 8th month and also they have significantly lower og calls when compared to non churn cutomers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### roam outgoing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['roam_og_mou_6','roam_og_mou_7','roam_og_mou_8']].describe(percentiles = [0.25,0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### std_og Vs churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "i = 1\n",
    "for col in ['std_og_mou_6','std_og_mou_7','std_og_mou_8']:\n",
    "    plt.subplot(1,3,i)\n",
    "    sns.boxplot(x = 'churn',y = col,data = telecom_hvc_6,showfliers = False)\n",
    "    i = i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['std_og_mou_6','std_og_mou_7','std_og_mou_8','churn']].groupby('churn').median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference -We can see that the non churn customers have almost similar median through the period where as for churn customers it is almost 0 in the 8th month and also we can see that the decrease is very huge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### roam outgoing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "telecom_hvc_6[['roam_og_mou_6','roam_og_mou_7','roam_og_mou_8']].describe(percentiles = [0.25,0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['roam_og_mou_6','roam_og_mou_7','roam_og_mou_8']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### roam_og Vs churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "i = 1\n",
    "for col in ['roam_og_mou_6','roam_og_mou_7','roam_og_mou_8']:\n",
    "    plt.subplot(1,3,i)\n",
    "    sns.violinplot(x = 'churn',y = col,data = telecom_hvc_6,showfliers = False)\n",
    "    i = i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_6[['roam_og_mou_6','roam_og_mou_7','roam_og_mou_8','churn']].groupby('churn').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference-The customers who have churned have large amount of std out going calls in the 6th month, but as with the progession their roam outgoing calls have also decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internet_cols = [col for col in telecom_hvc_6.columns if '_2g' in col or '_3g' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internet_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_7 = telecom_hvc_6.drop(internet_cols,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_7.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_7.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Univariate Analysis - Quantitative Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_plotting_variable(figure_title, xlabel, ylabel):\n",
    "    \n",
    "    plt.title(figure_title)\n",
    "    plt.xlabel(xlabel, labelpad = 15)\n",
    "    plt.ylabel(ylabel, labelpad = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_univariate(figsize_x, figsize_y, subplot_x, subplot_y, xlabel, ylabel, x_axis, data, wspace):\n",
    "    \n",
    "    plt.figure(figsize = (figsize_x, figsize_y))\n",
    "    \n",
    "    title_1 = \"Distribution Plot of \" + xlabel\n",
    "    title_2 = \"Box Plot of \" + xlabel\n",
    "\n",
    "    # Subplot - 1\n",
    "    plt.subplot(subplot_x, subplot_y, 1)\n",
    "\n",
    "    sns.distplot(data[x_axis], hist = True, kde = True, color = 'g')\n",
    "    # Call Custom Function\n",
    "    set_plotting_variable(title_1, xlabel, ylabel)\n",
    "\n",
    "    # Subplot - 2\n",
    "    plt.subplot(subplot_x, subplot_y, 2)\n",
    "\n",
    "    sns.boxplot(x = x_axis, data = data, color = 'm')\n",
    "    # Call Custom Function\n",
    "    set_plotting_variable(title_2, xlabel, ylabel)\n",
    "    \n",
    "    plt.subplots_adjust(wspace = wspace)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Plot Analysis of Quantitative Variables\n",
    "\n",
    "category_list = ['night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8', \n",
    "                 'fb_user_6', 'fb_user_7', 'fb_user_8', 'churn']\n",
    "counter = 1\n",
    "\n",
    "for col_list in telecom_hvc_7.columns:\n",
    "    \n",
    "    if col_list not in category_list:\n",
    "        \n",
    "        # Call Custom Function\n",
    "        plot_univariate(figsize_x = 20,\n",
    "                        figsize_y = 8,\n",
    "                        subplot_x = 1,\n",
    "                        subplot_y = 2,\n",
    "                        xlabel = col_list,\n",
    "                        ylabel = \"Distribution\",\n",
    "                        x_axis = col_list,\n",
    "                        data = telecom_hvc_7,\n",
    "                        wspace = 0.2)\n",
    "\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference-There are lot of Outliers present in the variables. We will remove these outliers by performaing normalization in the later stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "telecom_hvc_7[['arpu_6','arpu_7','arpu_8']].describe(percentiles = [0.25,0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_7[['arpu_6','arpu_7','arpu_8']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arpu Vs churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "i = 1\n",
    "for col in ['arpu_6','arpu_7','arpu_8']:\n",
    "    plt.subplot(1,3,i)\n",
    "    sns.violinplot(x = 'churn',y = col,data = telecom_hvc_7,showfliers = False)\n",
    "    i = i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_7[['arpu_6','arpu_7','arpu_8','churn']].groupby('churn').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference-The customers who have churned have large revenue in the 6th month, but as with the progession their revenue have also decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "telecom_hvc_7[['onnet_mou_6','onnet_mou_7','onnet_mou_8']].describe(percentiles = [0.25,0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_7[['onnet_mou_6','onnet_mou_7','onnet_mou_8']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### onnet Vs churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "i = 1\n",
    "for col in ['onnet_mou_6','onnet_mou_7','onnet_mou_8']:\n",
    "    plt.subplot(1,3,i)\n",
    "    sns.violinplot(x = 'churn',y = col,data = telecom_hvc_7,showfliers = False)\n",
    "    i = i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_7[['onnet_mou_6','onnet_mou_7','onnet_mou_8','churn']].groupby('churn').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference-The customers who have churned has a decreasing onnnet calls per month as we move from 6 to 8th months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "telecom_hvc_7[['offnet_mou_6','offnet_mou_7','offnet_mou_8']].describe(percentiles = [0.25,0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_7[['offnet_mou_6','offnet_mou_7','offnet_mou_8']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### offnet Vs churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "i = 1\n",
    "for col in ['offnet_mou_6','offnet_mou_7','offnet_mou_8']:\n",
    "    plt.subplot(1,3,i)\n",
    "    sns.violinplot(x = 'churn',y = col,data = telecom_hvc_7,showfliers = False)\n",
    "    i = i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_7[['offnet_mou_6','offnet_mou_7','offnet_mou_8','churn']].groupby('churn').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference-The customers who have churned has a decreasing offnet calls per month as we move from 6 to 8th months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is_rech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_7[['is_rech_6','is_rech_7','is_rech_8']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### is_rech Vs churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "i = 1\n",
    "for col in ['is_rech_6','is_rech_7','is_rech_8']:\n",
    "    plt.subplot(1,3,i)\n",
    "    sns.barplot(x = 'churn',y = col,data = telecom_hvc_7)\n",
    "    i = i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_7[['is_rech_6','is_rech_7','is_rech_8','churn']].groupby('churn').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference-The customers who have churned has stopped recharging their mobiles as the months progress from 6 to 8th which can be seen in decrease in is_rech values, which is a derived column from last_day_of_rech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_cols = [col for col in telecom_hvc_7.columns if '_ic' in col]\n",
    "ic_cols.append('churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (25,25))\n",
    "sns.heatmap(telecom_hvc_7[ic_cols].corr(),annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference-We can see that there are quite a few number of variables correlated and this is expected because the total variables are obtained from the indiavidual variables. We can also see that there is a high correlation among corresponding variables for different months.We can also expect similar trend among other variables as well. Let the model handle these correlated variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation with Churn variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = telecom_hvc_7.corr()['churn'].sort_values(ascending = False)[1:]\n",
    "plt.figure(figsize = (20,6))\n",
    "plt.bar(range(len(corr)),corr)\n",
    "plt.xticks(range(len(corr)),corr.index,rotation = 'vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference-We can see that the drop of og calls and not recharging the mobiles in the 8th seems to signify that the customer is going to churn. We can also see that the std and roam variables are also contributing to the churn positively where as max rech and total calls seems to effect the churn negatively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating difference variables and averaging the first two months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_8 = telecom_hvc_7.drop(['total_rech_num_6','total_rech_num_7','total_rech_num_8','total_rech_amt_6','total_rech_amt_7','total_rech_amt_8','og_others_6','av_amt_6_7','total_amt_6_7'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_8.drop([col for col in telecom_hvc_8.columns if 'diff_' in col],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_excempt = ['is_rech_6','is_rech_7','is_rech_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_8.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_cols = []\n",
    "seven_cols = []\n",
    "for x in telecom_hvc_8.columns:\n",
    "    if '_6' in x and x not in cols_to_excempt :\n",
    "        six_cols.append(x)\n",
    "    if '_7' in x and x not in cols_to_excempt :\n",
    "        seven_cols.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_cols1 = [x.replace('_6','') for x in six_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_9 = telecom_hvc_8.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in six_cols1:\n",
    "    out = 'av_'+x+'_6_7'\n",
    "    in_6 = x+'_6'\n",
    "    in_7 = x+'_7'\n",
    "    telecom_hvc_9[out] = (telecom_hvc_9[in_6]+telecom_hvc_9[in_7])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in six_cols1:\n",
    "    out1 = 'diff_'+x\n",
    "    in_av = 'av_'+x+'_6_7'\n",
    "    in_8 = x+'_8'\n",
    "    telecom_hvc_9[out1] = telecom_hvc_9[in_av]-telecom_hvc_9[in_8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_9.drop(six_cols+seven_cols,axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_9.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_hvc_9.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Handling class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = telecom_hvc_9.drop('churn',axis=1)\n",
    "y = telecom_hvc_9.churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y))\n",
    "print('\\n')\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size = 0.8,test_size = 0.2,random_state = 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a balanced X_train and y_train\n",
    "smote = SMOTE(random_state = 10)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_train))\n",
    "print('\\n')\n",
    "print(pd.Series(y_train).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the class imbalance is handled after SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Test train split and Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = []\n",
    "for x in range(1,len(X_train)+1):\n",
    "    index_list.append('I'+str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train,columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.index = index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.index = index_list\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train,X_test,y_train,y_test = train_test_split(X_balanced,y_balanced,train_size = 0.8,test_size = 0.2,stratify = y_balanced,random_state = 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.DataFrame(X_train_scaled,columns = X_train.columns)\n",
    "X_train_scaled.index = X_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled,columns = X_test.columns)\n",
    "X_test_scaled.index = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 - Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lr_1 = sm.add_constant(X_train_scaled)\n",
    "logmodel1 = sm.GLM(y_train,X_lr_1,family = sm.families.Binomial())\n",
    "logmodel1.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = logmodel1.fit().predict(X_lr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final = y_train.reset_index().rename(columns = {0:'churn'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final = y_train_pred_final.merge(y_train_pred.reset_index().rename(columns = {0:'churn_prob'}),on = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n",
    "                                              drop_intermediate = False )\n",
    "    auc_score = metrics.roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.churn, y_train_pred_final.churn_prob, drop_intermediate = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_roc(y_train_pred_final.churn, y_train_pred_final.churn_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimal cutoff point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create columns with different probability cutoffs \n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "for i in numbers:\n",
    "    y_train_pred_final[i]= y_train_pred_final.churn_prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci','prec'])\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# TP = confusion[1,1] # true positive \n",
    "# TN = confusion[0,0] # true negatives\n",
    "# FP = confusion[0,1] # false positives\n",
    "# FN = confusion[1,0] # false negatives\n",
    "\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for i in num:\n",
    "    cm1 = metrics.confusion_matrix(y_train_pred_final.churn, y_train_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    \n",
    "    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    prec = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci,prec]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot accuracy sensitivity and specificity for various probabilities.\n",
    "plt.figure(figsize = (10,5))\n",
    "cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\n",
    "plt.xticks(np.arange(0,0.95,0.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the curve above, 0.54 is the optimum point to take it as a cutoff probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final['final_predicted'] = y_train_pred_final.churn_prob.map( lambda x: 1 if x > 0.55 else 0)\n",
    "\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_sm = metrics.confusion_matrix(y_train_pred_final.churn, y_train_pred_final.final_predicted )\n",
    "confusion_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion_sm[1,1] # true positive \n",
    "TN = confusion_sm[0,0] # true negatives\n",
    "FP = confusion_sm[0,1] # false positives\n",
    "FN = confusion_sm[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our logistic regression model\n",
    "print('Sensitivity:', (TP / float(TP+FN)))\n",
    "\n",
    "# Let us calculate specificity\n",
    "print('Specificity:',(TN / float(TN+FP)))\n",
    "      \n",
    "#precision\n",
    "print('Precision:',(TP/float(FP+TP)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 PCA - Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(svd_solver='randomized', random_state=28)\n",
    "pca.fit(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = list(X_train_scaled.columns)\n",
    "pcs_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':colnames})\n",
    "pcs_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the screeplot - plotting the cumulative variance against the number of components\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize = (12,8))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.yticks(np.arange(0,1.05,0.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_final = IncrementalPCA(n_components=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_train_pca = pca_final.fit_transform(X_train_scaled)\n",
    "telecom_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_columns = []\n",
    "for x in range(1,21):\n",
    "    pc_columns.append('PC'+str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_pc_df = pd.DataFrame((telecom_train_pca),columns = pc_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pc_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pc_df.index = X_train_scaled.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Logistic regression with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_lr_1 = sm.add_constant(data_pc_df)\n",
    "pca_lm1 = sm.GLM(y_train,X_pca_lr_1,family = sm.families.Binomial())\n",
    "pca_lm1.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some of the P values are greater than 0.05.Hence dropping those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data_df = data_pc_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data_df.drop('PC20',axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_lr_2 = sm.add_constant(pca_data_df)\n",
    "pca_lm2 = sm.GLM(y_train,X_pca_lr_2,family = sm.families.Binomial())\n",
    "pca_lm2.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data_df.drop('PC17',axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_lr_3 = sm.add_constant(pca_data_df)\n",
    "pca_lm3 = sm.GLM(y_train,X_pca_lr_3,family = sm.families.Binomial())\n",
    "pca_lm3.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data_df.drop('PC4',axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_lr_4 = sm.add_constant(pca_data_df)\n",
    "pca_lm4 = sm.GLM(y_train,X_pca_lr_4,family = sm.families.Binomial())\n",
    "pca_lm4.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data_df.drop('PC19',axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_lr_5 = sm.add_constant(pca_data_df)\n",
    "pca_lm5 = sm.GLM(y_train,X_pca_lr_5,family = sm.families.Binomial())\n",
    "pca_lm5.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all the p values in this model are less than 0.05 hence this is a stable model. VIF check is not needed because pca creates un correlated variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_pca_lr_5.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_pca_lr_5.values, i) for i in range(X_pca_lr_5.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = pca_lm5.fit().predict(X_pca_lr_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final = y_train.reset_index().rename(columns = {0:'churn'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final = y_train_pred_final.merge(y_train_pred.reset_index().rename(columns = {0:'churn_prob'}),on = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n",
    "                                              drop_intermediate = False )\n",
    "    auc_score = metrics.roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.churn, y_train_pred_final.churn_prob, drop_intermediate = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_roc(y_train_pred_final.churn, y_train_pred_final.churn_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimal cutoff point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create columns with different probability cutoffs \n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "for i in numbers:\n",
    "    y_train_pred_final[i]= y_train_pred_final.churn_prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci','prec'])\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# TP = confusion[1,1] # true positive \n",
    "# TN = confusion[0,0] # true negatives\n",
    "# FP = confusion[0,1] # false positives\n",
    "# FN = confusion[1,0] # false negatives\n",
    "\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for i in num:\n",
    "    cm1 = metrics.confusion_matrix(y_train_pred_final.churn, y_train_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    \n",
    "    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    prec = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci,prec]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot accuracy sensitivity and specificity for various probabilities.\n",
    "plt.figure(figsize = (10,5))\n",
    "cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\n",
    "plt.xticks(np.arange(0,0.95,0.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the curve above, 0.52 is the optimum point to take it as a cutoff probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final['final_predicted'] = y_train_pred_final.churn_prob.map( lambda x: 1 if x > 0.52 else 0)\n",
    "\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the overall accuracy.\n",
    "metrics.accuracy_score(y_train_pred_final.churn, y_train_pred_final.final_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion2 = metrics.confusion_matrix(y_train_pred_final.churn, y_train_pred_final.final_predicted )\n",
    "confusion2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion2[1,1] # true positive \n",
    "TN = confusion2[0,0] # true negatives\n",
    "FP = confusion2[0,1] # false positives\n",
    "FN = confusion2[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our logistic regression model\n",
    "TP / float(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us calculate specificity\n",
    "TN / float(TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate false postive rate \n",
    "print(FP/ float(TN+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive predictive value \n",
    "print (TP / float(TP+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative predictive value\n",
    "print (TN / float(TN+ FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_test_pca = pca_final.transform(X_test_scaled)\n",
    "\n",
    "test_pc_df = pd.DataFrame((telecom_test_pca),columns = pc_columns)\n",
    "\n",
    "test_pc_df.index = X_test_scaled.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test_pca_lr = sm.add_constant(test_pc_df[pca_data_df.columns.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = pca_lm5.fit().predict(X_test_pca_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_final = y_test.reset_index().rename(columns = {0:'churn'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_final = y_test_pred_final.merge(y_test_pred.reset_index().rename(columns = {0:'churn_prob'}),on = 'mobile_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_final['final_predicted'] = y_test_pred_final.churn_prob.map( lambda x: 1 if x > 0.54 else 0)\n",
    "\n",
    "y_test_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the overall accuracy.\n",
    "metrics.accuracy_score(y_test_pred_final.churn, y_test_pred_final.final_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion2 = metrics.confusion_matrix(y_test_pred_final.churn, y_test_pred_final.final_predicted )\n",
    "confusion2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion2[1,1] # true positive \n",
    "TN = confusion2[0,0] # true negatives\n",
    "FP = confusion2[0,1] # false positives\n",
    "FN = confusion2[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our logistic regression model\n",
    "TP / float(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us calculate specificity\n",
    "TN / float(TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate false postive rate \n",
    "print(FP/ float(TN+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive predictive value \n",
    "print (TP / float(TP+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative predictive value\n",
    "print (TN / float(TN+ FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test score are consistent therfore the model is not overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Random forest with pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(data_pc_df,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = rfc.predict(data_pc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing confusion matrix\n",
    "print(confusion_matrix(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = rfc.predict(test_pc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing confusion matrix\n",
    "print(confusion_matrix(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'max_depth': [5,10,15,20],\n",
    "    'n_estimators': [500,1000]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(data_pc_df,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the optimal accuracy score and hyperparameters\n",
    "print('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_final = RandomForestClassifier(n_estimators = 1000,max_depth = 20,bootstrap = True,random_state = 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_final.fit(data_pc_df,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = rfc_final.predict(data_pc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing confusion matrix\n",
    "print(confusion_matrix(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = rfc_final.predict(test_pc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing confusion matrix\n",
    "print(confusion_matrix(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Factors contributing to Churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection using RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rfe = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe20 = RFE(lr_rfe, 20)             \n",
    "rfe20 = rfe20.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Selected columns from RFE:')\n",
    "print(X_train_scaled.columns[rfe20.support_].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train_scaled.columns[rfe20.support_].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation with 20 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model20 = sm.add_constant(X_train_scaled[cols])\n",
    "logmodel20 = sm.GLM(y_train,X_model20,family = sm.families.Binomial())\n",
    "logmodel20.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_scaled[cols].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_scaled[cols].values, i) for i in range(X_train_scaled[cols].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.remove('diff_loc_og_mou')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with 19 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model19 = sm.add_constant(X_train_scaled[cols])\n",
    "logmodel19 = sm.GLM(y_train,X_model19,family = sm.families.Binomial())\n",
    "logmodel19.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_scaled[cols].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_scaled[cols].values, i) for i in range(X_train_scaled[cols].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.remove('diff_arpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with 18 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model18 = sm.add_constant(X_train_scaled[cols])\n",
    "logmodel18 = sm.GLM(y_train,X_model18,family = sm.families.Binomial())\n",
    "logmodel18.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_scaled[cols].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_scaled[cols].values, i) for i in range(X_train_scaled[cols].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.remove('diff_total_ic_mou')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with 17 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model17 = sm.add_constant(X_train_scaled[cols])\n",
    "logmodel17 = sm.GLM(y_train,X_model17,family = sm.families.Binomial())\n",
    "logmodel17.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_scaled[cols].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_scaled[cols].values, i) for i in range(X_train_scaled[cols].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.remove('diff_total_og_mou')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with 16 Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model16 = sm.add_constant(X_train_scaled[cols])\n",
    "logmodel16 = sm.GLM(y_train,X_model16,family = sm.families.Binomial())\n",
    "logmodel16.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_scaled[cols].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_scaled[cols].values, i) for i in range(X_train_scaled[cols].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.remove('av_arpu_6_7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with 15 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model15 = sm.add_constant(X_train_scaled[cols])\n",
    "logmodel15 = sm.GLM(y_train,X_model15,family = sm.families.Binomial())\n",
    "logmodel15.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_scaled[cols].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_scaled[cols].values, i) for i in range(X_train_scaled[cols].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.remove('loc_ic_mou_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with 14 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model14 = sm.add_constant(X_train_scaled[cols])\n",
    "logmodel14 = sm.GLM(y_train,X_model14,family = sm.families.Binomial())\n",
    "logmodel14.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_scaled[cols].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_scaled[cols].values, i) for i in range(X_train_scaled[cols].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.remove('av_loc_og_mou_6_7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with 13 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model13 = sm.add_constant(X_train_scaled[cols])\n",
    "logmodel13 = sm.GLM(y_train,X_model13,family = sm.families.Binomial())\n",
    "logmodel13.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_scaled[cols].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_scaled[cols].values, i) for i in range(X_train_scaled[cols].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.remove('loc_og_t2m_mou_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model11 = sm.add_constant(X_train_scaled[cols])\n",
    "logmodel11 = sm.GLM(y_train,X_model11,family = sm.families.Binomial())\n",
    "logmodel11.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_scaled[cols].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_scaled[cols].values, i) for i in range(X_train_scaled[cols].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All p values are less than 0.05 and all VIFs are less than 5. Therefore this is stable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = logmodel11.fit().predict(X_model11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final = y_train.reset_index().rename(columns = {0:'churn'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final = y_train_pred_final.merge(y_train_pred.reset_index().rename(columns = {0:'churn_prob'}),on = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.churn, y_train_pred_final.churn_prob, drop_intermediate = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_roc(y_train_pred_final.churn, y_train_pred_final.churn_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimal cutoff point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create columns with different probability cutoffs \n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "for i in numbers:\n",
    "    y_train_pred_final[i]= y_train_pred_final.churn_prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci','prec'])\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# TP = confusion[1,1] # true positive \n",
    "# TN = confusion[0,0] # true negatives\n",
    "# FP = confusion[0,1] # false positives\n",
    "# FN = confusion[1,0] # false negatives\n",
    "\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for i in num:\n",
    "    cm1 = metrics.confusion_matrix(y_train_pred_final.churn, y_train_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    \n",
    "    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    prec = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci,prec]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot accuracy sensitivity and specificity for various probabilities.\n",
    "plt.figure(figsize = (10,5))\n",
    "cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\n",
    "plt.xticks(np.arange(0,0.95,0.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the curve above, 0.54 is the optimum point to take it as a cutoff probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final['final_predicted'] = y_train_pred_final.churn_prob.map( lambda x: 1 if x > 0.61 else 0)\n",
    "\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's check the overall accuracy.\n",
    "metrics.accuracy_score(y_train_pred_final.churn, y_train_pred_final.final_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion2 = metrics.confusion_matrix(y_train_pred_final.churn, y_train_pred_final.final_predicted )\n",
    "confusion2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion2[1,1] # true positive \n",
    "TN = confusion2[0,0] # true negatives\n",
    "FP = confusion2[0,1] # false positives\n",
    "FN = confusion2[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our logistic regression model\n",
    "TP / float(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us calculate specificity\n",
    "TN / float(TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate false postive rate \n",
    "print(FP/ float(TN+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive predictive value \n",
    "print (TP / float(TP+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative predictive value\n",
    "print (TN / float(TN+ FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = logmodel11.fit().predict(sm.add_constant(X_test_scaled[cols]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_final = y_test.reset_index().rename(columns = {0:'churn'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_final = y_test_pred_final.merge(y_test_pred.reset_index().rename(columns = {0:'churn_prob'}),on = 'mobile_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_final['final_predicted'] = y_test_pred_final.churn_prob.map( lambda x: 1 if x > 0.54 else 0)\n",
    "\n",
    "y_test_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the overall accuracy.\n",
    "metrics.accuracy_score(y_test_pred_final.churn, y_test_pred_final.final_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion2 = metrics.confusion_matrix(y_test_pred_final.churn, y_test_pred_final.final_predicted )\n",
    "confusion2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion2[1,1] # true positive \n",
    "TN = confusion2[0,0] # true negatives\n",
    "FP = confusion2[0,1] # false positives\n",
    "FN = confusion2[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our logistic regression model\n",
    "TP / float(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us calculate specificity\n",
    "TN / float(TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate false postive rate \n",
    "print(FP/ float(TN+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive predictive value \n",
    "print (TP / float(TP+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative predictive value\n",
    "print (TN / float(TN+ FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test score are consistent therfore the model is not overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logmodel11.fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__churn = -7.1867    +16.8248\\*`roam_og_mou_8` -19.6774 \\*`loc_og_t2f_mou_8 ` -54.5052\\*`loc_og_mou_8 ` -5.6667  \\*`total_og_mou_8 ` -24.6461\\*`total_ic_mou_8` -7.5832 \\*`spl_ic_mou_8` -21.8440 \\*`last_day_rch_amt_8`   -14.8813 \\*`total_rech_count_8` + 9.6326 \\*`av_roam_ic_mou_6_7` +  3.0978 \\*`av_std_og_t2m_mou_6_7` + 18.3178\\*`diff_loc_ic_mou`__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random forest with PCA has better prediction capability than the logistic regression with accuracy of 0.94 and 0.95 sensitivity\n",
    "- `loc_og_t2f_mou_8 `,`total_ic_mou_8`,`last_day_rch_amt_8`,`spl_ic_mou_8`,`total_rech_count_8 `,`roam_og_mou_8` are effecting the churn negatively decreasing the probability of churn and `diff_loc_ic_mou`,`av_roam_ic_mou_6_7`,`av_loc_og_t2m_mou_6_7` are effecting the churn positively, increasing the probability of churn.\n",
    "- From this we can say that std and roam variables are causing the churn, which may mean that competition are offering better rates, which may be resulting in churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        - We can lower the roaming charges as this is one of the reasons for pre-paid customers to seek out local operators when on roaming.\n",
    "        -Also customize pre-paid plan offers can be issued: For example customized plans based on where customer minutes of usage are higher - for instance outgoing/std/t2t/t2m etc .  \n",
    "        -For customers who are staying longer with network we can increase customer loyalty points and offer some special discounts.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
